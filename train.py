import os
# TensorFlowì˜ ë¡œê·¸ ë ˆë²¨ì„ ì„¤ì •í•˜ì—¬ INFO ë° WARNING ë©”ì‹œì§€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
# '2'ëŠ” INFOì™€ WARNINGì„ ëª¨ë‘ ìˆ¨ê¸°ëŠ” ë ˆë²¨ì…ë‹ˆë‹¤.
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import math
import multiprocessing
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split, Subset
from tqdm import tqdm
import matplotlib.pyplot as plt

# ë¶„ë¦¬ëœ ëª¨ë¸ ë° ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
import model_config as config
from model import PeptideDataset, DiffusionTrainer, get_model
# ìƒˆë¡œ ì¶”ê°€ëœ ë¶„ì„ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸ (ê²½ë¡œ ìˆ˜ì •)
from utils.analysis_utils import run_validation_metrics

# ==============================================================================
# í•™ìŠµ ìœ í‹¸ë¦¬í‹°
# ==============================================================================
class EarlyStopping:
    """
    ê²€ì¦ ì†ì‹¤ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œì‹œí‚¤ëŠ” í´ë˜ìŠ¤.
    Overfittingì„ ë°©ì§€í•˜ê³  ìµœì ì˜ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    def __init__(self, patience=30, min_delta=1e-5, save_path=None):
        """
        Args:
            patience (int): ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šì•„ë„ ì°¸ì„ ì—í­ ìˆ˜.
            min_delta (float): ê°œì„ ë˜ì—ˆë‹¤ê³  íŒë‹¨í•  ìµœì†Œ ì†ì‹¤ ê°ì†ŒëŸ‰.
            save_path (str): ìµœì  ëª¨ë¸ì˜ state_dictê°€ ì €ì¥ë  ê²½ë¡œ.
        """
        self.patience, self.min_delta, self.save_path = patience, min_delta, save_path
        self.best_loss, self.counter = float('inf'), 0

    def step(self, val_loss, epoch, model, optimizer, scheduler):
        """ë§¤ ì—í­ì˜ ê²€ì¦ ë‹¨ê³„ê°€ ëë‚œ í›„ í˜¸ì¶œë©ë‹ˆë‹¤."""
        # í˜„ì¬ ê²€ì¦ ì†ì‹¤ì´ ê¸°ë¡ëœ ìµœì  ì†ì‹¤ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë‚®ì€ ê²½ìš°
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0 # ì¹´ìš´í„° ì´ˆê¸°í™”
            # ìµœì  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            if self.save_path and model:
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                    'best_loss': self.best_loss,
                }
                torch.save(checkpoint, self.save_path)
                print(f"   -> Saved checkpoint for epoch {epoch}. Best loss: {self.best_loss:.4f}")
        else:
            # ê°œì„ ë˜ì§€ ì•Šì€ ê²½ìš° ì¹´ìš´í„° ì¦ê°€
            self.counter += 1
        
        # ì¹´ìš´í„°ê°€ ì„¤ì •ëœ patienceë¥¼ ì´ˆê³¼í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ ì‹ í˜¸(True) ë°˜í™˜
        return self.counter >= self.patience

def build_warmup_cosine_scheduler(optimizer, total_epochs, warmup_ratio=0.05):
    """
    ê°„ë‹¨í•œ Warmup í›„ Cosine Decayë¥¼ ì ìš©í•˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    warmup_epochs = max(1, int(total_epochs * warmup_ratio))
    
    def lr_lambda(epoch_idx):
        if epoch_idx < warmup_epochs:
            # Warmup: 0 -> 1
            return float(epoch_idx + 1) / float(warmup_epochs)
        else:
            # Cosine Decay: 1 -> 0
            progress = (epoch_idx - warmup_epochs) / max(1, (total_epochs - warmup_epochs))
            return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def_wandb = False
try:
    import wandb
except ImportError:
    def_wandb = False


# ==============================================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# ==============================================================================
def train_diffusion():
    """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜."""
    # --- 1. ê¸°ë³¸ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ---
    device = config.MISC["device"]
    B = config.TRAINING_PARAMS["batch_size"]
    print(f"Using device: {device}")

    # --- 1a. Wandb ì´ˆê¸°í™” (ì„ íƒì ) ---
    if config.MISC.get("wandb_project"):
        try:
            import wandb
            wandb.init(
                project=config.MISC["wandb_project"],
                entity=config.MISC.get("wandb_entity"),
                name=config.MISC.get("wandb_run_name"),
                config=config.to_dict() # ëª¨ë“  ì„¤ì •ì„ wandbì— ê¸°ë¡
            )
            def_wandb = True
        except ImportError:
            print("wandb not installed, skipping log. Please run 'pip install wandb'")
            def_wandb = False
    else:
        def_wandb = False

    # --- 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ---
    # ë°ì´í„°ì…‹ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    train_path = config.PATHS["train_dataset"]
    test_path = config.PATHS["test_dataset"]
    if not os.path.exists(train_path) or not os.path.exists(test_path):
        print(f"ğŸš¨ Error: Dataset file not found. Checked paths: {train_path}, {test_path}")
        return None, None

    # PeptideDataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±
    train_ds = PeptideDataset(train_path)
    val_ds = PeptideDataset(test_path)
    # DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
    train_loader = DataLoader(train_ds, batch_size=B, shuffle=True, num_workers=config.MISC["num_workers"], pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=B, shuffle=False, num_workers=config.MISC["num_workers"], pin_memory=True, drop_last=True)

    # --- 3. ëª¨ë¸ ë° EarlyStopping ì´ˆê¸°í™” ---
    model = get_model(config)
    model.to(device)  # 1. ëª¨ë¸ì„ ë¨¼ì € GPUë¡œ ë³´ëƒ…ë‹ˆë‹¤.

    # EarlyStopping ê°ì²´ë¥¼ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
    early_stopper = EarlyStopping(
        patience=config.TRAINING_PARAMS["early_stop_patience"],
        save_path=config.PATHS["save_path"]
    )

    # --- ì¶”ê°€: ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---
    start_epoch = 0
    save_path = config.PATHS["save_path"]
    optimizer_state_dict, scheduler_state_dict = None, None

    if os.path.exists(save_path):
        print(f"Resuming training from checkpoint: {save_path}")
        try:
            checkpoint = torch.load(save_path, map_location=device)
            
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                from collections import OrderedDict
                new_state_dict = OrderedDict()
                for k, v in state_dict.items():
                    name = k.replace('module.', '')
                    new_state_dict[name] = v
                
                model.load_state_dict(new_state_dict)
                
                optimizer_state_dict = checkpoint.get('optimizer_state_dict')
                scheduler_state_dict = checkpoint.get('scheduler_state_dict')
                start_epoch = checkpoint['epoch']
                
                # ìƒì„±ëœ early_stopper ì¸ìŠ¤í„´ìŠ¤ì˜ best_loss ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                early_stopper.best_loss = checkpoint.get('best_loss', float('inf'))

                print(f"   -> Resuming from epoch {start_epoch + 1}. Best validation loss was {early_stopper.best_loss:.4f}.")
            else:
                model.load_state_dict(checkpoint)
                print("   -> Loaded an old format checkpoint (only model state_dict). Starting from epoch 0.")

        except Exception as e:
            print(f"Error loading checkpoint: {e}. Starting from scratch.")
    else:
        print("No checkpoint found, starting training from scratch.")

    # --- 3a. Multi-GPU ì„¤ì • ---
    if config.MISC.get("multi_gpu", False) and torch.cuda.device_count() > 1:
        print(f"ğŸš€ Let's use {torch.cuda.device_count()} GPUs!")
        model = torch.nn.DataParallel(model)

    # 4. ëª¨ë¸ì´ GPUì— ìœ„ì¹˜í•œ í›„ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    optimizer = optim.AdamW(model.parameters(), lr=config.TRAINING_PARAMS["learning_rate"], weight_decay=0.01)
    scheduler = build_warmup_cosine_scheduler(
        optimizer,
        total_epochs=config.TRAINING_PARAMS["epochs"],
        warmup_ratio=config.TRAINING_PARAMS["warmup_ratio"]
    )

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì½ì–´ì˜¨ ìƒíƒœê°€ ìˆìœ¼ë©´ ë¡œë“œí•©ë‹ˆë‹¤.
    if optimizer_state_dict:
        optimizer.load_state_dict(optimizer_state_dict)
    if scheduler_state_dict:
        scheduler.load_state_dict(scheduler_state_dict)
    
    # 5. ëª¨ë¸ì´ ìµœì¢… ë””ë°”ì´ìŠ¤ì— ìœ„ì¹˜í•œ í›„ DiffusionTrainerë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    diffusion = DiffusionTrainer(model) 

    train_losses, val_losses = [], []

    # --- 4. í•™ìŠµ ë£¨í”„ ì‹œì‘ ---
    print("ğŸš€ Starting training...")
    for epoch in range(start_epoch, config.TRAINING_PARAMS["epochs"]):
        model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“± í™œì„±í™”)
        total_train_loss = 0.0

        # tqdmì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        for batch_data in progress_bar:
            # --- 4a. ë°ì´í„° ì¤€ë¹„ (ìˆ˜ì •ë¨) ---
            # PeptideDatasetì˜ __getitem__ ë°˜í™˜ í˜•ì‹ì— ë§ê²Œ ì–¸íŒ¨í‚¹ (ì¡°ê±´ì´ dict í˜•íƒœë¡œ ë°˜í™˜ë¨)
            x0_cpu, cond_dict_cpu, seq_mask_cpu, feat_mask_cpu, lengths_cpu, real_bond_lengths_cpu = batch_data
            
            x0 = x0_cpu.to(device, non_blocking=True)
            cond_dict = {k: v.to(device, non_blocking=True) for k, v in cond_dict_cpu.items()}
            seq_mask = seq_mask_cpu.to(device, non_blocking=True)
            feat_mask = feat_mask_cpu.to(device, non_blocking=True)
            lengths = lengths_cpu.to(device, non_blocking=True)
            real_bond_lengths = real_bond_lengths_cpu.to(device, non_blocking=True)
            
            # --- 4b. ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ (ìˆ˜ì •ë¨) ---
            # model.pyì˜ training_loss ì‹œê·¸ë‹ˆì²˜ì™€ ì¸ì ìˆœì„œë¥¼ ì •í™•í•˜ê²Œ ì¼ì¹˜ì‹œí‚´
            total_loss, loss_info = diffusion.training_loss(
                x0=x0,
                c=cond_dict, # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì¡°ê±´ ì „ë‹¬
                seq_len_mask=seq_mask,
                feat_valid_mask=feat_mask,
                lengths=lengths,
                real_bond_lengths=real_bond_lengths,
                cond_prob=config.TRAINING_PARAMS["condition_dropout"],
                current_epoch=epoch + 1 # í˜„ì¬ ì—í­ ë²ˆí˜¸ ì „ë‹¬
            )
            
            optimizer.zero_grad()
            total_loss.backward()
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì¶”ê°€ (í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            optimizer.step()
            
            # --- 5. ë¡œê¹… ---
            total_train_loss += loss_info['total_loss']
            
            # ë¡œê·¸ì— í‘œì‹œí•  FAPE ì†ì‹¤ ë¬¸ìì—´ í¬ë§·íŒ…
            fape_log_str = f"{loss_info.get('raw_fape_loss', 0.0):.4f} (w: {loss_info.get('fape_loss', 0.0):.4f})"
            feat_weight = loss_info.get('feat_weight', 1.0)
            fape_weight = loss_info.get('fape_weight', 0.0)
            ca_only_flag = int(loss_info.get('fape_use_ca_only', 0))
            
            progress_bar.set_postfix({
                "loss": loss_info['total_loss'], 
                "feat_loss": loss_info['feat_loss'],
                "fape_loss(raw|w)": fape_log_str,
                "feat_w": feat_weight,
                "fape_w": fape_weight,
                "ca_only": ca_only_flag
            })
            if def_wandb:
                # ìŠ¤í…ë³„ë¡œ ë” ìƒì„¸í•œ ë¡œê·¸ë¥¼ wandbì— ê¸°ë¡
                log_data = {
                    "step_loss": loss_info['total_loss'],
                    "step_feat_loss": loss_info['feat_loss'],
                    "step_raw_fape_loss": loss_info.get('raw_fape_loss', 0.0),
                    "step_fape_loss_weighted": loss_info.get('fape_loss', 0.0),
                    "feat_loss_weight": feat_weight,
                    "fape_loss_weight": fape_weight,
                    "fape_use_ca_only": ca_only_flag,
                    "fape_local_k": loss_info.get('fape_local_k', -1),
                    "learning_rate": scheduler.get_last_lr()[0]
                }
                wandb.log(log_data)
            
        # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # --- Validation ---
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch_data in val_loader:
                # PeptideDatasetì˜ __getitem__ ë°˜í™˜ í˜•ì‹ì— ë§ê²Œ ì–¸íŒ¨í‚¹
                x0_cpu, cond_dict_cpu, seq_mask_cpu, feat_mask_cpu, lengths_cpu, real_bond_lengths_cpu = batch_data
                x0 = x0_cpu.to(device)
                cond_dict = {k: v.to(device) for k, v in cond_dict_cpu.items()}
                seq_mask = seq_mask_cpu.to(device)
                feat_mask = feat_mask_cpu.to(device)
                lengths = lengths_cpu.to(device)
                real_bond_lengths = real_bond_lengths_cpu.to(device)
                
                # ê²€ì¦ ì‹œì—ëŠ” ramp_scaleê³¼ cond_probì„ ê³ ì •ê°’ìœ¼ë¡œ ì‚¬ìš©
                _, loss_info = diffusion.training_loss(
                    x0=x0,
                    c=cond_dict, # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì¡°ê±´ ì „ë‹¬
                    seq_len_mask=seq_mask,
                    feat_valid_mask=feat_mask,
                    lengths=lengths,
                    real_bond_lengths=real_bond_lengths,
                    cond_prob=1.0, # í•­ìƒ ì»¨ë””ì…˜ ì‚¬ìš©
                    current_epoch=epoch + 1 # ê²€ì¦ ì‹œì—ë„ ì—í­ ì „ë‹¬ (ê°€ì¤‘ì¹˜ëŠ” ë‚´ë¶€ì—ì„œ ê²°ì •)
                )
                total_val_loss += loss_info['total_loss']

        scheduler.step()
        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0
        current_lr = scheduler.get_last_lr()[0]
        val_losses.append(avg_val_loss)
        
        # --- ì¶”ê°€ëœ ê²€ì¦ ë©”íŠ¸ë¦­ ì‹¤í–‰ ---
        avg_rmsd = -1.0 # ê¸°ë³¸ê°’
        # --- ìˆ˜ì •: ê²€ì¦ ë©”íŠ¸ë¦­ ì‹¤í–‰ ì¡°ê±´ì„ FAPE Loss ê³„ì‚° ì‹œì‘ ì‹œì ê³¼ ë™ê¸°í™” ---
        is_last_epoch = (epoch + 1) == config.TRAINING_PARAMS["epochs"]
        
        # (ìˆ˜ì •) ë§¤ 100 ì—í¬í¬ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ ì—í¬í¬ì— ë©”íŠ¸ë¦­ ì‹¤í–‰
        should_run_metrics = ((epoch + 1) % 100 == 0)

        if should_run_metrics or is_last_epoch:
            print(f"\nRunning validation metrics (Backbone RMSD, Ramachandran) for epoch {epoch+1}...")
            avg_rmsd = run_validation_metrics(
                model, diffusion, val_loader, epoch + 1
            )
        
        # ì—í­ ê²°ê³¼ ì¶œë ¥ (RMSD ì¶”ê°€)
        print(
            f"[Epoch {epoch+1}/{config.TRAINING_PARAMS['epochs']}] "
            f"LR: {current_lr:.2e} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | "
            f"Val RMSD: {avg_rmsd:.4f}"
        )

        # --- Wandb ì—í­ ë¡œê·¸ ---
        if def_wandb:
            wandb.log({
                "epoch": epoch + 1,
                "avg_train_loss": avg_train_loss,
                "avg_val_loss": avg_val_loss,
                "avg_val_rmsd": avg_rmsd,
                "learning_rate_epoch": current_lr
            })

        # --- 6. ì¡°ê¸° ì¢…ë£Œ í™•ì¸ ---
        if early_stopper.step(avg_val_loss, epoch + 1, model, optimizer, scheduler):
            print(f"Early stopping at epoch {epoch+1}. Best Val Loss: {early_stopper.best_loss:.4f}")
            break

    # --- 7. í•™ìŠµ ì¢…ë£Œ í›„ ê²°ê³¼ ì €ì¥ ---
    print("âœ… Training finished.")
    # ì†ì‹¤ ê³¡ì„  ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
    try:
        plt.figure(figsize=(10, 5))
        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')
        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')
        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Diffusion Training Loss Curve')
        plt.legend(); plt.grid(True)
        plt.tight_layout()
        plt.savefig("diffusion_training_loss_curve.png", dpi=150)
        print("Saved loss curve to diffusion_training_loss_curve.png")
    except Exception as e:
        print(f"Could not save loss plot: {e}")

    return model, diffusion

# ==============================================================================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§€ì 
# ==============================================================================
if __name__ == '__main__':
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œì‘ ë°©ë²• ì„¤ì • (Windows/macOS í˜¸í™˜ì„± ë¬¸ì œ ë°©ì§€)
    try:
        multiprocessing.set_start_method('spawn')
    except RuntimeError:
        pass
    
    # ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ í˜¸ì¶œ
    trained_model, diffusion_trainer = train_diffusion()
