"""
íŒŒì¼ ì´ë¦„: create_dataset.py
ì„¤ëª…: Bio.PDB ê¸°ë°˜ìœ¼ë¡œ ë°±ë³¸(N, CA, C)ë§Œì„ ì‚¬ìš©í•´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
      - ì‚¬ì´ë“œì²´ì¸ í”¼ì²˜ëŠ” ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
      - ë°±ë³¸ ê°ë„(Ï†/Ïˆ/Ï‰), ê²°í•©ê°, ê²°í•©ê±°ë¦¬, ë°±ë³¸ ì¢Œí‘œ(N,CA,C)ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
"""
import numpy as np
import os
import torch
import esm
import pickle
import sys
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from Bio.PDB import PDBParser, is_aa
from sklearn.model_selection import train_test_split
import pandas as pd

# --- 1. ì„¤ì • ì„í¬íŠ¸ ---
try:
    import create_dataset_config as config
except ImportError:
    print("âŒ ì˜¤ë¥˜: 'create_dataset_config.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
    sys.exit(1)


# --- 2. ê¸°í•˜ ìœ í‹¸ë¦¬í‹° (NumPy ê¸°ë°˜) ---
def _unit(v):
    """ë²¡í„°ë¥¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    n = np.linalg.norm(v)
    return v / n if n > 1e-8 else v

def _angle(a, b, c):
    """ì„¸ ì  a, b, c ì‚¬ì´ì˜ ê°ë„(âˆ abc)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    v1 = _unit(a - b)
    v2 = _unit(c - b)
    dot = np.clip(np.dot(v1, v2), -1.0, 1.0)
    return float(np.arccos(dot))

def _dihedral(p1, p2, p3, p4):
    """ë„¤ ì  p1, p2, p3, p4 ì‚¬ì´ì˜ ë¹„í‹€ë¦¼ê°(dihedral)ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    b0 = p2 - p1
    b1 = p3 - p2
    b2 = p4 - p3
    b1_u = _unit(b1)
    v = b0 - np.dot(b0, b1_u) * b1_u
    w = b2 - np.dot(b2, b1_u) * b1_u
    x = np.dot(_unit(v), _unit(w))
    y = np.dot(np.cross(b1_u, _unit(v)), _unit(w))
    return float(np.arctan2(y, x))


# --- 3. ë°±ë³¸ íŠ¹ì§• ê³„ì‚° í´ë˜ìŠ¤ ---
class BackboneFeatureCalculator:
    """Bio.PDBë¥¼ ì‚¬ìš©í•˜ì—¬ PDB íŒŒì¼ë¡œë¶€í„° ë°±ë³¸ ê´€ë ¨ í”¼ì²˜ë§Œ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤."""
    def __init__(self):
        self.parser = PDBParser(QUIET=True)

    def _pick_chain(self, structure, pdb_path):
        """PDB êµ¬ì¡°ì—ì„œ ê°€ì¥ ê¸´ ì•„ë¯¸ë…¸ì‚° ì²´ì¸ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        models = list(structure.get_models())
        if not models:
            tqdm.write(f"  - âš ï¸ {os.path.basename(pdb_path)} íŒŒì¼ì—ì„œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        model = models[0] # IDì— ìƒê´€ì—†ì´ ì²« ë²ˆì§¸ ëª¨ë¸ì„ ì‚¬ìš©

        chains = list(model.get_chains())
        if not chains: return None

        best_chain, best_len = None, -1
        for ch in chains:
            residues = [r for r in ch.get_residues() if is_aa(r, standard=False)]
            if len(residues) > best_len:
                best_len = len(residues)
                best_chain = ch
        return best_chain

    def calculate_sequence_and_features(self, pdb_path: str):
        """PDB íŒŒì¼ë¡œë¶€í„° ì„œì—´ê³¼ ë°±ë³¸ í”¼ì²˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            structure = self.parser.get_structure(Path(pdb_path).stem, pdb_path)
        except Exception as e:
            tqdm.write(f"  - âš ï¸ Bio.PDB íŒŒì‹± ì˜¤ë¥˜ {os.path.basename(pdb_path)}: {e}")
            return None, None

        chain = self._pick_chain(structure, pdb_path)
        if chain is None: return None, None

        # N, CA, C ë°±ë³¸ ì›ìê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì•„ë¯¸ë…¸ì‚° ì”ê¸°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
        residues = []
        for r in chain.get_residues():
            if is_aa(r, standard=False) and 'N' in r and 'CA' in r and 'C' in r:
                residues.append(r)
        
        L = len(residues)
        if L == 0: return None, None

        # --- í”¼ì²˜ ê³„ì‚° ---
        seq3 = [r.get_resname().strip() for r in residues]
        n = np.stack([r['N'].coord.astype(np.float32) for r in residues])
        ca = np.stack([r['CA'].coord.astype(np.float32) for r in residues])
        c = np.stack([r['C'].coord.astype(np.float32) for r in residues])

        # 1. ë°±ë³¸ ì¢Œí‘œ (L, 3, 3)
        backbone_coords = np.stack([n, ca, c], axis=1)

        # 2. ê²°í•© ê±°ë¦¬ (L, 3): N-CA, CA-C, C-N(+1)
        bb_dist = np.full((L, 3), np.nan, dtype=np.float32)
        bb_dist[:, 0] = np.linalg.norm(n - ca, axis=1)
        bb_dist[:, 1] = np.linalg.norm(ca - c, axis=1)
        bb_dist[:-1, 2] = np.linalg.norm(c[:-1] - n[1:], axis=1)

        # 3. ê²°í•© ê° (L, 3): âˆ N-CA-C, âˆ CA-C-N(+1), âˆ C(-1)-N-CA
        bb_ang = np.full((L, 3), np.nan, dtype=np.float32)
        for i in range(L):
            bb_ang[i, 0] = _angle(n[i], ca[i], c[i])
            if i < L - 1: bb_ang[i, 1] = _angle(ca[i], c[i], n[i+1])
            if i > 0: bb_ang[i, 2] = _angle(c[i-1], n[i], ca[i])

        # 4. Torsions (L, 3): Ï†, Ïˆ, Ï‰
        bb_tor = np.full((L, 3), np.nan, dtype=np.float32)
        for i in range(L):
            if i > 0: bb_tor[i, 0] = _dihedral(c[i-1], n[i], ca[i], c[i])
            if i < L - 1: bb_tor[i, 1] = _dihedral(n[i], ca[i], c[i], n[i+1])
            if i > 0: bb_tor[i, 2] = _dihedral(ca[i-1], c[i-1], n[i], ca[i])

        # ìµœì¢… í”¼ì²˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        features_dict = {}
        if config.CALCULATE_BB_TORSION_ANGLES: features_dict["bb_torsions"] = bb_tor
        if config.CALCULATE_BB_BOND_ANGLES: features_dict["bb_angles"] = bb_ang
        if config.CALCULATE_BB_BOND_DISTANCES: features_dict["bb_distances"] = bb_dist
        if config.CALCULATE_BACKBONE_COORDS: features_dict["backbone_coords"] = backbone_coords
        
        return seq3, features_dict

# --- 4. ESM ëª¨ë¸ ë¡œë“œ ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

esm_model, alphabet = None, None
if config.CALCULATE_ESM_EMBEDDING:
    print(f"Loading ESM-2 model ({config.ESM_MODEL_NAME})...")
    esm_model, alphabet = esm.pretrained.load_model_and_alphabet(config.ESM_MODEL_NAME)
    batch_converter = alphabet.get_batch_converter()
    esm_model.eval().to(device)
    if str(device) == 'cuda': esm_model.half()
    print("ESM-2 model loaded.")

# --- 5. í—¬í¼ í•¨ìˆ˜ ---
ALL_AMINO_ACIDS = sorted(list(config.CANONICAL_AMINO_ACIDS))
AA_MAP_20D = {aa: i for i, aa in enumerate(ALL_AMINO_ACIDS)}
AA3_TO_1 = {
    "ALA":"A", "CYS":"C", "ASP":"D", "GLU":"E", "PHE":"F", "GLY":"G", "HIS":"H",
    "ILE":"I", "LYS":"K", "LEU":"L", "MET":"M", "ASN":"N", "PRO":"P", "GLN":"Q",
    "ARG":"R", "SER":"S", "THR":"T", "VAL":"V", "TRP":"W", "TYR":"Y"
}

def encode_sequence(seq_list, max_len, aa_map):
    encoding = np.zeros((max_len, len(aa_map)), dtype=np.float32)
    for i, aa in enumerate(seq_list):
        if i >= max_len: break
        if aa in aa_map: encoding[i, aa_map[aa]] = 1.0
    return encoding

def pad_array(arr, target_len, pad_value=np.nan):
    current_len = arr.shape[0]
    if current_len >= target_len: return arr[:target_len]
    pad_shape = (target_len - current_len,) + arr.shape[1:]
    pad = np.full(pad_shape, pad_value, dtype=arr.dtype)
    return np.concatenate([arr, pad], axis=0)

def get_esm_embedding(batch_samples, layers):
    _batch_labels, batch_strs, batch_tokens = batch_converter(batch_samples)
    batch_tokens = batch_tokens.to(device)
    with torch.no_grad():
        results = esm_model(batch_tokens, repr_layers=[layers], return_contacts=False)
    token_representations = results["representations"][layers]
    embedding_list = []
    for i, (_, seq_str) in enumerate(batch_samples):
        seq_len = len(seq_str)
        embedding_list.append(token_representations[i, 1 : seq_len + 1, :].cpu())
    return embedding_list

# --- 6. ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜ ---
def create_dataset(pdb_root_dir, feature_calculator):
    dataset = {'pdb_file': [], 'sequences_3letter': [], 'sequence_1letter': [], 'sequence_encoded': []}
    # ì„¤ì •ì— ë”°ë¼ ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬ í‚¤ ì´ˆê¸°í™”
    if config.CALCULATE_BB_TORSION_ANGLES: dataset["bb_torsions"] = []
    if config.CALCULATE_BB_BOND_ANGLES: dataset["bb_angles"] = []
    if config.CALCULATE_BB_BOND_DISTANCES: dataset["bb_distances"] = []
    if config.CALCULATE_SC_TORSION_ANGLES: dataset["sc_torsions"] = []
    if config.CALCULATE_BACKBONE_COORDS: dataset["backbone_coords"] = []
    if config.CALCULATE_ESM_EMBEDDING: dataset["esm_embedding"] = []
    
    pdb_directory = Path(pdb_root_dir)
    if not pdb_directory.exists():
        print(f"âŒ PDB ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdb_directory}")
        return dataset

    pdb_files = sorted([f for f in pdb_directory.glob("*.pdb")])
    skipped_files_info = {}

    pbar = tqdm(range(0, len(pdb_files), config.BATCH_SIZE), desc=f"Processing {pdb_directory.name}")
    for start in pbar:
        batch_pdb = pdb_files[start : start + config.BATCH_SIZE]
        batch_meta_info = []

        for pdb_file in batch_pdb:
            seq_list, features_dict = feature_calculator.calculate_sequence_and_features(str(pdb_file))
            
            if seq_list is None:
                skipped_files_info[pdb_file] = "ìœ íš¨ ë°±ë³¸ ì”ê¸° ì—†ìŒ/ì²˜ë¦¬ ì˜¤ë¥˜"
                continue
            if not (0 < len(seq_list) <= config.MAX_LEN):
                skipped_files_info[pdb_file] = f"ê¸¸ì´ í•„í„°ë§ë¨ (ê¸¸ì´: {len(seq_list)})"
                continue

            batch_meta_info.append({
                "pdb_file": pdb_file, "seq_list": seq_list, "features": features_dict
            })

        if not batch_meta_info: continue
            
        if config.CALCULATE_ESM_EMBEDDING:
            try:
                batch_samples_for_esm = [
                    (info["pdb_file"].name, "".join(AA3_TO_1.get(aa, "X") for aa in info["seq_list"]))
                    for info in batch_meta_info
                ]
                batch_embeddings = get_esm_embedding(batch_samples_for_esm, config.ESM_REPR_LAYER)
                for info, emb in zip(batch_meta_info, batch_embeddings):
                    info["esm_embedding"] = emb
            except RuntimeError as e:
                print(f"\nESM ì„ë² ë”© ì˜¤ë¥˜ (ë°°ì¹˜ ê±´ë„ˆëœ€): {e}", file=sys.stderr)
                for info in batch_meta_info: skipped_files_info[info['pdb_file']] = "ESM ì²˜ë¦¬ ì˜¤ë¥˜"
                continue

        for info in batch_meta_info:
            dataset["pdb_file"].append(str(info["pdb_file"]))
            dataset["sequences_3letter"].append(info["seq_list"])
            
            seq_1letter = "".join(AA3_TO_1.get(aa, "X") for aa in info["seq_list"])
            dataset["sequence_1letter"].append(seq_1letter)
            dataset["sequence_encoded"].append(encode_sequence(info["seq_list"], config.MAX_LEN, AA_MAP_20D))
            
            for key, arr in info["features"].items():
                dataset[key].append(pad_array(arr, config.MAX_LEN))
            
            if config.CALCULATE_ESM_EMBEDDING and "esm_embedding" in info:
                dataset["esm_embedding"].append(pad_array(info["esm_embedding"].numpy().astype(np.float16), config.MAX_LEN))

    if skipped_files_info:
        with open("create_dataset_error.log", "a", encoding="utf-8") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"--- Log at {timestamp} for [{pdb_directory.name}] ---\n")
            for fname, reason in sorted(skipped_files_info.items()):
                f.write(f"  - íŒŒì¼: {fname}, ì›ì¸: {reason}\n")
            f.write("-" * 50 + "\n\n")

    return dataset


def split_and_save_dataset(full_dataset, output_dir, filename_prefix):
    """ë°ì´í„°ì…‹ì„ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì¸µí™”í•˜ì—¬ train/validationìœ¼ë¡œ ë¶„í• í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    if not any(full_dataset.values()):
        print("âš ï¸ ë¶„í• í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, None

    # ë°ì´í„°ì…‹ì˜ ëª¨ë“  ê°’ì„ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ë£¨ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.
    df = pd.DataFrame(full_dataset)
    # ê³„ì¸µí™”ì˜ ê¸°ì¤€ì´ ë  ê° í©íƒ€ì´ë“œì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ìƒˆ ì—´ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    df['length'] = df['sequences_3letter'].apply(len)
    
    # ê° ê¸¸ì´ë³„ ìƒ˜í”Œ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    length_counts = df['length'].value_counts()
    
    # ìƒ˜í”Œì´ 1ê°œë¿ì¸ 'ì™¸í†¨ì´' ê¸¸ì´ë“¤ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
    lone_sample_lengths = length_counts[length_counts < 2].index.tolist()
    
    if lone_sample_lengths:
        print(f"[*] ì •ë³´: ìƒ˜í”Œì´ 1ê°œë¿ì¸ ê¸¸ì´ë“¤ {sorted(lone_sample_lengths)} ì€(ëŠ”) ëª¨ë‘ í›ˆë ¨ ë°ì´í„°ì…‹ì— í¬í•¨ë©ë‹ˆë‹¤.")
        # ì™¸í†¨ì´ ìƒ˜í”Œê³¼ ë¶„í•  ê°€ëŠ¥í•œ ìƒ˜í”Œë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë‚˜ëˆ•ë‹ˆë‹¤.
        df_lone = df[df['length'].isin(lone_sample_lengths)]
        df_splittable = df[~df['length'].isin(lone_sample_lengths)]
    else:
        # ì™¸í†¨ì´ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ë¶„í•  ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.
        df_lone = pd.DataFrame()
        df_splittable = df

    # ë¶„í•  ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ê³„ì¸µ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    if not df_splittable.empty:
        train_df_main, val_df = train_test_split(
            df_splittable,
            test_size=config.VALIDATION_SPLIT_RATIO,
            stratify=df_splittable['length'],  # ê¸¸ì´ ë¶„í¬ë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€
            random_state=42    # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
        )
    else:
        # ë¶„í•  ê°€ëŠ¥í•œ ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° (ëª¨ë“  ìƒ˜í”Œì´ ì™¸í†¨ì´ì¸ ê²½ìš°)
        train_df_main, val_df = pd.DataFrame(), pd.DataFrame()

    # ì™¸í†¨ì´ ìƒ˜í”Œë“¤ì„ í›ˆë ¨ ë°ì´í„°ì…‹ì— í•©ì¹©ë‹ˆë‹¤.
    train_df = pd.concat([train_df_main, df_lone], ignore_index=True)
    
    # ë” ì´ìƒ í•„ìš”ì—†ëŠ” 'length' ì—´ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    train_df = train_df.drop(columns=['length'])
    val_df = val_df.drop(columns=['length'], errors='ignore') # val_dfê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜¤ë¥˜ ë¬´ì‹œ

    # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    train_dataset = {col: train_df[col].tolist() for col in train_df.columns}
    val_dataset = {col: val_df[col].tolist() for col in val_df.columns}

    # ë¶„í• ëœ ë°ì´í„°ì…‹ì„ ê°ê°ì˜ .pkl íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_path = output_dir / f"{filename_prefix}_train.pkl"
    with open(train_path, 'wb') as f:
        pickle.dump(train_dataset, f)
    print(f"==> âœ… Train dataset saved to {train_path} ({len(train_df)} samples)")

    val_path = None
    if not val_df.empty:
        val_path = output_dir / f"{filename_prefix}_val.pkl"
        with open(val_path, 'wb') as f:
            pickle.dump(val_dataset, f)
        print(f"==> âœ… Validation dataset saved to {val_path} ({len(val_df)} samples)")
        
    return str(train_path), str(val_path) if val_path else None

# --- 7. ë°ì´í„°ì…‹ ê²€ì¦ í•¨ìˆ˜ ---
def verify_dataset_entry(dataset_path: str):
    print("\n--- ë°ì´í„°ì…‹ ê²€ì¦ ì‹œì‘ ---")
    if not Path(dataset_path).exists():
        print(f"âŒ ê²€ì¦í•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        return

    print(f"ğŸ” '{dataset_path}' íŒŒì¼ì˜ ì²« ë²ˆì§¸ í•­ëª©ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
    try:
        with open(dataset_path, 'rb') as f: data = pickle.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return

    if not isinstance(data, dict) or not all(isinstance(v, list) for v in data.values()):
        print("âŒ ë°ì´í„° í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. (key: list of values í˜•íƒœê°€ ì•„ë‹˜)")
        return

    first_entry_data = {key: (value_list[0] if value_list else "N/A") for key, value_list in data.items()}
    
    print("\n[ì²« ë²ˆì§¸ ìƒ˜í”Œ ë°ì´í„°]")
    for key, value in first_entry_data.items():
        if isinstance(value, np.ndarray): print(f"- {key}: shape={value.shape}, dtype={value.dtype}")
        elif isinstance(value, list): print(f"- {key}: list of {len(value)} items")
        else: print(f"- {key}: {value}")
    print("\n--- ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ ---\n")

# --- 8. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    feature_calculator = BackboneFeatureCalculator()

    print(f"\nğŸš€ Starting dataset creation in '{config.DATASET_MODE}' mode.")
    if config.DATASET_MODE == 'train_test':
        print("\n[1/2] ì „ì²´ í•™ìŠµ/ê²€ì¦ìš© PDBë¡œë¶€í„° í”¼ì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        # ìš°ì„  ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        full_train_val_dataset = create_dataset(config.PDB_TRAIN_DIR, feature_calculator)
        
        print("\n[2/2] ë°ì´í„°ì…‹ì„ Train/Validationìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤...")
        train_pkl_path, val_pkl_path = split_and_save_dataset(
            full_train_val_dataset,
            config.DATASET_OUTPUT_DIR,
            config.OUTPUT_FILENAME_PREFIX
        )

        print("\n[+] TEST ë°ì´í„°ì…‹ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        test_dataset = create_dataset(config.PDB_TEST_DIR, feature_calculator)

        test_pkl_path = None
        if test_dataset and any(len(v) > 0 for v in test_dataset.values()):
            test_path = config.DATASET_OUTPUT_DIR / f"{config.OUTPUT_FILENAME_PREFIX}_test.pkl"
            config.DATASET_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            with open(test_path, 'wb') as f: pickle.dump(test_dataset, f)
            print(f"==> âœ… Test dataset saved to {test_path}")
            test_pkl_path = str(test_path)
        
        if test_pkl_path: verify_dataset_entry(test_pkl_path)
            
    elif config.DATASET_MODE == '10_fold':
        print("10-fold ëª¨ë“œëŠ” í˜„ì¬ ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print(f"âŒ Unknown mode in config: {config.DATASET_MODE}")
