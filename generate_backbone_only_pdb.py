# íŒŒì¼ ì´ë¦„: generate_backbone_only_pdb.py
# ì„¤ëª…: í•™ìŠµëœ Diffusion ëª¨ë¸ê³¼ NERF ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ í©íƒ€ì´ë“œ ì„œì—´ë¡œë¶€í„° 3D ë°±ë³¸ êµ¬ì¡°(PDB)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
#      PyRosetta ì˜ì¡´ì„±ì„ ì œê±°í•˜ê³ , ì˜ˆì¸¡ëœ ê°ë„ì™€ ê²°í•© ê¸¸ì´ë¥¼ ì§ì ‘ 3D ì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

import os
# TensorFlowì˜ ë¡œê·¸ ë ˆë²¨ì„ ì„¤ì •í•˜ì—¬ INFO ë° WARNING ë©”ì‹œì§€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import pickle
import numpy as np
import torch
from tqdm import tqdm
from pathlib import Path
from torch.cuda.amp import autocast

# --- 1. ì„¤ì • ë° ëª¨ë¸/ë°ì´í„° í´ë˜ìŠ¤ ì„í¬íŠ¸ ---
import model_config as train_config
import generate_backbone_only_pdb_config as gen_config
from model import DiffusionTrainer, PeptideDataset, get_model
from utils.nerf import nerf_build_batch

# --- 2. PDB íŒŒì¼ ì €ì¥ ìœ í‹¸ë¦¬í‹° ---
def save_coords_to_pdb(coords: np.ndarray, sequence_3_letter: list, output_path: str):
    """
    ì£¼ì–´ì§„ 3D ì¢Œí‘œì™€ ì•„ë¯¸ë…¸ì‚° ì„œì—´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ PDB íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Args:
        coords (np.ndarray): ì›ì ì¢Œí‘œ ë°°ì—´. shape: (num_atoms, 3)
        sequence_3_letter (list): 3-letter ì•„ë¯¸ë…¸ì‚° ì½”ë“œì˜ ë¦¬ìŠ¤íŠ¸.
        output_path (str): ì €ì¥ë  PDB íŒŒì¼ì˜ ê²½ë¡œ.
    """
    atom_names = ["N", "CA", "C"]
    num_residues = len(sequence_3_letter)
    
    with open(output_path, "w") as f:
        atom_idx = 1
        for res_idx in range(num_residues):
            res_name = sequence_3_letter[res_idx]
            for i in range(3):  # N, CA, C
                atom_name = atom_names[i]
                coord_idx = res_idx * 3 + i
                if coord_idx >= len(coords):
                    tqdm.write(f"âš ï¸ ê²½ê³ : ì¢Œí‘œ ë°°ì—´ì˜ ëì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. {res_name} ì”ê¸°ì˜ ì¼ë¶€ ì›ìê°€ ëˆ„ë½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    break
                
                x, y, z = coords[coord_idx]
                
                # PDB ATOM record format
                line = (
                    f"ATOM  {atom_idx:5d} {atom_name:<4s}{res_name:3s} A{res_idx+1:4d}    "
                    f"{x:8.3f}{y:8.3f}{z:8.3f}  1.00 20.00           {atom_name[0]:>2s}  \n"
                )
                f.write(line)
                atom_idx += 1
        f.write("TER\n")

# --- 3. ë©”ì¸ ìƒì„± í•¨ìˆ˜ ---
def generate_pdbs():
    """í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ NERF ê¸°ë°˜ PDB ìƒì„±ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜."""
    
    # --- ì„¤ì • ë¡œë“œ ---
    # ì¥ì¹˜ ì„ íƒ ìµœì í™”: 'cuda' ìš”ì²­ ì‹œ ê°€ëŠ¥í•˜ë©´ GPU ì‚¬ìš©, 'auto' ì§€ì›
    cfg_device = str(gen_config.GENERATION_PARAMS.get("device", "cpu")).lower()
    if cfg_device in ("cuda", "gpu", "cuda:0", "auto") and torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    # [ìµœì í™”] CUDA í™˜ê²½ì—ì„œ ì»¤ë„ ìë™ íŠœë‹ ë° matmul ì •ë°€ë„ í–¥ìƒ
    if device.type == "cuda":
        try:
            torch.backends.cudnn.benchmark = True
            torch.set_float32_matmul_precision("high")
        except Exception:
            pass
    model_path = Path(gen_config.PATHS["trained_model_path"])
    dataset_path = Path(gen_config.PATHS["dataset_path"])
    output_dir = Path(gen_config.PATHS["output_pdb_dir"])
    
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Generated PDBs will be saved in: {output_dir.resolve()}")

    # --- 2. ëª¨ë¸ ë¡œë“œ ---
    # ë¨¼ì € CPUì—ì„œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    model = get_model(train_config)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (CPU ë§¤í•‘)
    checkpoint_path = gen_config.GENERATION_PARAMS["model_checkpoint_path"]
    if not os.path.exists(checkpoint_path):
        print(f"ğŸš¨ Fatal Error: Model checkpoint not found at {checkpoint_path}")
        return
        
    print(f"Loading model checkpoint from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # state_dict ì¶”ì¶œ ë° 'module.' ì ‘ë‘ì‚¬ ì²˜ë¦¬ (DataParallel í˜¸í™˜ì„±)
    state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
    from collections import OrderedDict
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k.replace('module.', '') # 'module.' ì ‘ë‘ì‚¬ ì œê±°
        new_state_dict[name] = v
    
    model.load_state_dict(new_state_dict)
    print("Model loaded successfully.")

    # --- 2a. Multi-GPU ì„¤ì • ë° ì¥ì¹˜ë¡œ ëª¨ë¸ ì´ë™ ---
    if gen_config.GENERATION_PARAMS.get("multi_gpu", False) and torch.cuda.device_count() > 1:
        print(f"ğŸš€ Let's use {torch.cuda.device_count()} GPUs for generation!")
        model = torch.nn.DataParallel(model)
    model.to(device)
    
    model.eval()
    diffusion = DiffusionTrainer(model)

    # --- 3. ë°ì´í„°ì…‹ ì¤€ë¹„ ---
    test_dataset_path = gen_config.GENERATION_PARAMS["test_dataset_path"]
    
    # [ìˆ˜ì •] PeptideDataset ê°ì²´ ìƒì„±ì´ ëˆ„ë½ëœ ë¶€ë¶„ ì¶”ê°€
    if not os.path.exists(test_dataset_path):
        print(f"ğŸš¨ Fatal Error: Dataset file not found at {test_dataset_path}")
        return
    dataset = PeptideDataset(test_dataset_path)

    # ë°°ì¹˜ ìƒì„±ìœ¼ë¡œ ê°€ì†í™”
    gen_batch_size = int(gen_config.GENERATION_PARAMS.get("batch_size", 8))
    loader_workers = int(gen_config.GENERATION_PARAMS.get("num_workers", 0))
    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=gen_batch_size,
        shuffle=False,
        drop_last=False,
        num_workers=loader_workers,
        pin_memory=(device.type == "cuda")
    )
    print(f"âœ… Loaded dataset for conditions from '{test_dataset_path}'.")

    # --- PDB ìƒì„± ë£¨í”„ ---
    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc="Generating PDBs")
    
    num_samples_per_sequence = gen_config.GENERATION_PARAMS["num_samples_per_sequence"]
    # [ìµœì í™”] ìƒ˜í”Œë§ ì²­í¬ ì‚¬ì´ì¦ˆ ë¡œë“œ
    samples_batch_size = gen_config.GENERATION_PARAMS.get("samples_batch_size", 50) # ê¸°ë³¸ê°’ 50
    # [ìµœì í™”] ìë™ í˜¼í•©ì •ë°€ë„ ì„¤ì •
    precision = str(gen_config.GENERATION_PARAMS.get("precision", "fp32")).lower()
    autocast_dtype = None
    autocast_enabled = False
    if device.type == "cuda":
        if precision == "fp16":
            autocast_dtype = torch.float16
            autocast_enabled = True
        elif precision == "bf16":
            autocast_dtype = torch.bfloat16
            autocast_enabled = True

    # [ìµœì í™”] ë™ì  í”¼ì²˜ ì¸ë±ì‹±(ê³ ì •) â€” ë£¨í”„ ë°–ì—ì„œ ê³„ì‚°
    feature_indices = {}
    current_idx = 0
    for key in train_config.FEATURE_KEYS:
        feature_indices[key] = slice(current_idx, current_idx + train_config.FEATURE_DIMS[key])
        current_idx += train_config.FEATURE_DIMS[key]

    def get_feature(tensor, key):
        return tensor[:, :, feature_indices.get(key)]

    for batch_idx, batch_data in pbar:
        # --- ì›ë³¸ ë°ì´í„° ì¤€ë¹„ ---
        _, cond_dict_cpu, seq_mask_cpu, _, length_tensor, bond_len_cpu = batch_data
        original_batch_size = seq_mask_cpu.shape[0]

        # [ìµœì í™”] ìƒ˜í”Œë§ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰
        for s_chunk_start in range(0, num_samples_per_sequence, samples_batch_size):
            s_chunk_end = min(s_chunk_start + samples_batch_size, num_samples_per_sequence)
            current_samples_in_chunk = s_chunk_end - s_chunk_start

            # [ìµœì í™”] ì²­í¬ ì „ì²´ê°€ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if gen_config.GENERATION_PARAMS.get("skip_existing", False):
                all_exist = True
                for original_item_idx in range(original_batch_size):
                    global_item_idx = batch_idx * original_batch_size + original_item_idx
                    if 'pdb_file' in dataset.data and global_item_idx < len(dataset.data['pdb_file']):
                        pdb_id = Path(dataset.data['pdb_file'][global_item_idx]).stem
                    else:
                        pdb_id = f"idx{global_item_idx}"
                    prefix = gen_config.GENERATION_PARAMS.get("filename_prefix", "gen")
                    for sample_sub_idx in range(current_samples_in_chunk):
                        global_sample_idx = s_chunk_start + sample_sub_idx
                        out_path = output_dir / f"{prefix}_{pdb_id}_sample_{global_sample_idx}.pdb"
                        if not out_path.exists():
                            all_exist = False
                            break
                    if not all_exist:
                        break
                if all_exist:
                    continue

            # --- [ìµœì í™”] í˜„ì¬ ì²­í¬ì— ë§ê²Œ ì¡°ê±´ í…ì„œ í™•ì¥ ---
            mega_batch_size = original_batch_size * current_samples_in_chunk
            cond_dict = {
                k: v.repeat_interleave(current_samples_in_chunk, dim=0).to(device, non_blocking=True)
                for k, v in cond_dict_cpu.items()
            }
            seq_mask = seq_mask_cpu.repeat_interleave(current_samples_in_chunk, dim=0).to(device, non_blocking=True)

            # --- ê²°í•© ê¸¸ì´ í…ì„œ ì¤€ë¹„ (ì„¤ì •ì— ë”°ë¼ ë¶„ê¸°) ---
            if gen_config.GENERATION_PARAMS.get("use_ideal_bond_lengths", False):
                # ì´ìƒì ì¸ ê²°í•© ê¸¸ì´ ì‚¬ìš©
                max_len = train_config.DATA_SPECS["max_len"]
                ideal_lengths = torch.tensor([
                    gen_config.IDEAL_BOND_LENGTHS["N_CA_LENGTH"],
                    gen_config.IDEAL_BOND_LENGTHS["CA_C_LENGTH"],
                    gen_config.IDEAL_BOND_LENGTHS["C_N_LENGTH"],
                ], device=device)
                bond_len = ideal_lengths.view(1, 1, 3).expand(mega_batch_size, max_len, -1)
            else:
                # ë°ì´í„°ì…‹ì˜ ì‹¤ì¸¡ ê²°í•© ê¸¸ì´ ì‚¬ìš©
                bond_len = bond_len_cpu.repeat_interleave(current_samples_in_chunk, dim=0).to(device, non_blocking=True)
            
            # --- [ìµœì í™”] í™•ì¥ëœ ë°°ì¹˜ë¡œ ëª¨ë¸ ìƒ˜í”Œë§ (ë‹¨ í•œë²ˆ í˜¸ì¶œ) ---
            shape = (mega_batch_size, train_config.DATA_SPECS["max_len"], diffusion.feature_input_dim)
            with torch.no_grad():
                with autocast(dtype=autocast_dtype, enabled=autocast_enabled):
                    predicted_x0 = diffusion.sample(
                        cond_dict,
                        guidance_scale=gen_config.GENERATION_PARAMS["guidance_scale"],
                        shape=shape,
                        mask=seq_mask
                    )

            # --- ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥ (ì´í›„ ë¡œì§ì€ ë™ì¼) ---
            bb_torsions = get_feature(predicted_x0, 'bb_torsions')
            bb_angles = get_feature(predicted_x0, 'bb_angles')
            coords_batch = nerf_build_batch(
                phi=bb_torsions[:, :, 0], psi=bb_torsions[:, :, 1], omega=bb_torsions[:, :, 2],
                bond_angle_n_ca_c=bb_angles[:, :, 0], bond_angle_ca_c_n=bb_angles[:, :, 1], bond_angle_c_n_ca=bb_angles[:, :, 2],
                bond_len_n_ca=bond_len[:, :, 0], bond_len_ca_c=bond_len[:, :, 1], bond_len_c_n=bond_len[:, :, 2]
            ).detach().cpu().numpy()

            # ë°°ì¹˜ì˜ ê° í•­ëª©ì— ëŒ€í•´ íŒŒì¼ ì €ì¥
            for i in range(mega_batch_size):
                original_item_idx = i // current_samples_in_chunk
                sample_sub_idx = i % current_samples_in_chunk
                
                global_sample_idx = s_chunk_start + sample_sub_idx
                global_item_idx = batch_idx * original_batch_size + original_item_idx
                
                seq_len = int(length_tensor[original_item_idx].item())

                if 'pdb_file' in dataset.data and global_item_idx < len(dataset.data['pdb_file']):
                    pdb_id = Path(dataset.data['pdb_file'][global_item_idx]).stem
                else:
                    pdb_id = f"idx{global_item_idx}"

                sequence_3_letter = dataset.data['sequences_3letter'][global_item_idx][:seq_len]
                pbar.set_postfix_str(f"{pdb_id}_sample_{global_sample_idx}")
                coords_sample = coords_batch[i, :seq_len * 3, :]

                try:
                    prefix = gen_config.GENERATION_PARAMS.get("filename_prefix", "gen")
                    output_filename = f"{prefix}_{pdb_id}_sample_{global_sample_idx}.pdb"
                    out_path = output_dir / output_filename
                    if gen_config.GENERATION_PARAMS.get("skip_existing", False) and out_path.exists():
                        continue
                    # [ìµœì í™”] íŒŒì¼ ì“°ê¸° ìµœì†Œí™” â€” ë¬¸ìì—´ ëˆ„ì  í›„ ì¼ê´„ ì“°ê¸°
                    atom_names = ["N", "CA", "C"]
                    lines = []
                    for res_idx in range(seq_len):
                        res_name = sequence_3_letter[res_idx]
                        base = res_idx * 3
                        for i_atom in range(3):
                            atom_name = atom_names[i_atom]
                            coord_idx = base + i_atom
                            x, y, z = coords_sample[coord_idx]
                            lines.append(
                                f"ATOM  {len(lines)+1:5d} {atom_name:<4s}{res_name:3s} A{res_idx+1:4d}    "
                                f"{x:8.3f}{y:8.3f}{z:8.3f}  1.00 20.00           {atom_name[0]:>2s}  \n"
                            )
                    lines.append("TER\n")
                    with open(out_path, "w") as f:
                        f.writelines(lines)
                except Exception as e:
                    tqdm.write(f"â€¼ï¸ ERROR generating PDB for {pdb_id}_sample_{global_sample_idx}: {e}")

    print("\nâœ… Generation complete.")

# --- 4. ì‹¤í–‰ ---
if __name__ == "__main__":
    generate_pdbs()
